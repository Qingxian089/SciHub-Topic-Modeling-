{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb759968-08f2-4f1d-a400-68a6b9635d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Python interpreter will be restarted.\n",
       "Requirement already satisfied: pyLDAvis in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (3.3.1)\n",
       "Requirement already satisfied: scipy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (1.4.1)\n",
       "Requirement already satisfied: gensim in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (4.2.0)\n",
       "Requirement already satisfied: future in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (0.18.2)\n",
       "Requirement already satisfied: setuptools in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (45.2.0.post20200210)\n",
       "Requirement already satisfied: numpy&gt;=1.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (1.21.6)\n",
       "Requirement already satisfied: pandas&gt;=1.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (1.3.5)\n",
       "Requirement already satisfied: jinja2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (2.11.1)\n",
       "Requirement already satisfied: funcy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (1.17)\n",
       "Requirement already satisfied: joblib in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (0.14.1)\n",
       "Requirement already satisfied: scikit-learn in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (1.0.2)\n",
       "Requirement already satisfied: sklearn in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (0.0)\n",
       "Requirement already satisfied: numexpr in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (2.8.4)\n",
       "Requirement already satisfied: smart-open&gt;=1.8.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from gensim-&gt;pyLDAvis) (6.2.0)\n",
       "Requirement already satisfied: python-dateutil&gt;=2.7.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pandas&gt;=1.2.0-&gt;pyLDAvis) (2.8.1)\n",
       "Requirement already satisfied: pytz&gt;=2017.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pandas&gt;=1.2.0-&gt;pyLDAvis) (2019.3)\n",
       "Requirement already satisfied: MarkupSafe&gt;=0.23 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from jinja2-&gt;pyLDAvis) (1.1.1)\n",
       "Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from scikit-learn-&gt;pyLDAvis) (3.1.0)\n",
       "Requirement already satisfied: six&gt;=1.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=1.2.0-&gt;pyLDAvis) (1.14.0)\n",
       "Python interpreter will be restarted.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Python interpreter will be restarted.\nRequirement already satisfied: pyLDAvis in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (3.3.1)\nRequirement already satisfied: scipy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (1.4.1)\nRequirement already satisfied: gensim in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (4.2.0)\nRequirement already satisfied: future in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (0.18.2)\nRequirement already satisfied: setuptools in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (45.2.0.post20200210)\nRequirement already satisfied: numpy&gt;=1.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (1.21.6)\nRequirement already satisfied: pandas&gt;=1.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (1.3.5)\nRequirement already satisfied: jinja2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (2.11.1)\nRequirement already satisfied: funcy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (1.17)\nRequirement already satisfied: joblib in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (0.14.1)\nRequirement already satisfied: scikit-learn in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (1.0.2)\nRequirement already satisfied: sklearn in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (0.0)\nRequirement already satisfied: numexpr in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pyLDAvis) (2.8.4)\nRequirement already satisfied: smart-open&gt;=1.8.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from gensim-&gt;pyLDAvis) (6.2.0)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pandas&gt;=1.2.0-&gt;pyLDAvis) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from pandas&gt;=1.2.0-&gt;pyLDAvis) (2019.3)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from jinja2-&gt;pyLDAvis) (1.1.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from scikit-learn-&gt;pyLDAvis) (3.1.0)\nRequirement already satisfied: six&gt;=1.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=1.2.0-&gt;pyLDAvis) (1.14.0)\nPython interpreter will be restarted.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd7f001c-965e-4836-8d80-360585e25714",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# `pyLDAvis.sklearn`\n",
    "\n",
    "pyLDAvis now also supports LDA application from scikit-learn. Let's take a look into this in more detail. We will be using the 20 newsgroups dataset as provided by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e8e2ff1-988d-4ae1-9ef6-3da5e3b2ac22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf041ddc-3ab0-430c-8189-d111691371de",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses\n",
       "  from imp import reload\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/fft/__init__.py:97: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
       "  from numpy.dual import register_func\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
       "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
       "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
       "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
       "/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
       "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
       "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module&#39;s documentation for alternative uses\n  from imp import reload\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/fft/__init__.py:97: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n  from numpy.dual import register_func\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aac4e9f4-6ddd-41eb-8887-cf254b298e78",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/io/matlab/mio5.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
       "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
       "  from .mio5_utils import VarReader5\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-01ad131a-2b1c-49c4-9f43-421583ee28f9/lib/python3.7/site-packages/scipy/io/matlab/mio5.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  from .mio5_utils import VarReader5\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c695404-69d3-4197-9345-53b2a3ab31fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load 20 newsgroups dataset\n",
    "\n",
    "First, the 20 newsgroups dataset available in sklearn is loaded. As always, the headers, footers and quotes are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f636e765-f3fb-4b0a-87a8-bdc328b69d9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">&lt;&gt;:7: DeprecationWarning: invalid escape sequence \\[\n",
       "&lt;&gt;:16: DeprecationWarning: invalid escape sequence \\d\n",
       "&lt;&gt;:7: DeprecationWarning: invalid escape sequence \\[\n",
       "&lt;&gt;:16: DeprecationWarning: invalid escape sequence \\d\n",
       "&lt;command-150296596912317&gt;:7: DeprecationWarning: invalid escape sequence \\[\n",
       "  REPLACE_BY_SPACE_RE = re.compile(&#39;[/(){}\\[\\]\\|@,;.]&#39;)\n",
       "&lt;command-150296596912317&gt;:16: DeprecationWarning: invalid escape sequence \\d\n",
       "  text = re.sub(&#39;\\d+&#39;,&#39;&#39;,text)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">&lt;&gt;:7: DeprecationWarning: invalid escape sequence \\[\n&lt;&gt;:16: DeprecationWarning: invalid escape sequence \\d\n&lt;&gt;:7: DeprecationWarning: invalid escape sequence \\[\n&lt;&gt;:16: DeprecationWarning: invalid escape sequence \\d\n&lt;command-150296596912317&gt;:7: DeprecationWarning: invalid escape sequence \\[\n  REPLACE_BY_SPACE_RE = re.compile(&#39;[/(){}\\[\\]\\|@,;.]&#39;)\n&lt;command-150296596912317&gt;:16: DeprecationWarning: invalid escape sequence \\d\n  text = re.sub(&#39;\\d+&#39;,&#39;&#39;,text)\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import re\n",
    "# Read datasets/papers.csv into papers\n",
    "medicine = pd.read_csv(\"/dbfs/mnt/lsde/group14/area/math.csv\", header = 0, lineterminator='\\n').reset_index()\n",
    "# Remove punctuation\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;.]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "SHORTWORD = re.compile(r'\\W*\\b\\w{1,4}\\b')\n",
    "\n",
    "def clean_text(text):\n",
    "    #text = str(text).lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) \n",
    "    text = BAD_SYMBOLS_RE.sub('', text) \n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b','',str(text))\n",
    "    text = re.sub('\\d+','',text)\n",
    "    text = SHORTWORD.sub('',text)\n",
    "\n",
    "    return text\n",
    "medicine['content_processed'] = medicine['content'].astype(str).map(lambda x: clean_text (x))\n",
    "long_string = \" \".join(medicine.content_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e975330a-916d-40c5-a5ba-076a643754eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Convert to document-term matrix\n",
    "\n",
    "Next, the raw documents are converted into document-term matrix, possibly as raw counts or in TF-IDF form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fd7ad613-8f63-472c-bfd5-853af5c24398",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Series([], Name: content_processed, dtype: object)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Series([], Name: content_processed, dtype: object)\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "useless_list = ['omputersandathematicswithpplications', 'ournalofomputationalandppliedathematics', 'owever', 'niversity','ngineering', 'cience', 'omputer', 'nstitute', 'ystems', 'omputerouplingofhaseiagramsandhermochemistry', 'onlinearnalysisybridystems', \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",\"method\",\"university\",\"science\",\"equations\", \"ection\", 'uetal', 'paper', 'nthispaper', 'efinition']\n",
    "num_list = []\n",
    "for i in range(0, 2023):\n",
    "    num_list.append(str(i))\n",
    "\n",
    "max_features = 1000\n",
    "ngram_range = (1,2)\n",
    "lowercase = True\n",
    "    \n",
    "stopwords.extend(num_list)\n",
    "stopwords.extend(useless_list)\n",
    "print(medicine['content_processed'])\n",
    "from sklearn.feature_extraction import text\n",
    "#Convert a collection of raw documents to a matrix of TF features.\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\\\n",
    "                                stop_words = stopwords,\\\n",
    "                                max_features=max_features,\\\n",
    "                                ngram_range=ngram_range,\\\n",
    "                                lowercase = lowercase,\\\n",
    "                                max_df = 0.5, \\\n",
    "                                min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(medicine['content_processed'])\n",
    "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(medicine['content_processed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ff5d68d-7411-438d-b3cd-0083316f6573",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fit Latent Dirichlet Allocation models\n",
    "\n",
    "Finally, the LDA models are fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bea4cb94-95fa-4944-ba9e-5e075593229c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_components=3, random_state=0)\n",
    "lda_tf.fit(dtm_tf)\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=3, random_state=0)\n",
    "lda_tfidf.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c8ad913-6207-43bd-a008-eaaf589158d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Visualizing the models with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1be1d7f-6d45-425c-a4bf-bb4d9c2095c3",
     "showTitle": false,
     "title": ""
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d7e07fb1-50a2-445a-beaa-776bcd61b8fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tfidf, dtm_tfidf, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d9dd2b1f-81d8-4104-812e-4b7f10ec97a8",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "source": [
    "### Using different MDS functions\n",
    "\n",
    "With `sklearn` installed, other MDS functions, such as MMDS and TSNE can be used for plotting if the default PCoA is not satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ee1642b-a12f-4d4d-a473-ae39907057e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer, mds='mmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "949135ce-b29d-4f21-ae7c-922199074664",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer, mds='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ac327986-3fc1-4d35-b781-7c348fa8a2d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "lda_output = lda_tf.fit_transform(dtm_tf)\n",
    "#set topic numbers to each topic\n",
    "topicnames = [\"Topic\" + str(i) for i in range(lda_tf.n_components)]\n",
    "df_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames)\n",
    "#Calculate the probability that the paper corresponds to each topic.\n",
    "dominant_topic = np.argmax(df_topic.values, axis=1)\n",
    "#put the topic number into the dataframe\n",
    "df_topic['dominant_topic'] = dominant_topic\n",
    "print(df_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "496ad0f0-898b-4912-971a-5f7540c14216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#give the topic number to each paper\n",
    "medicine['topic'] = df_topic['dominant_topic']\n",
    "#drop unneed columns\n",
    "medicine = medicine.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'index'])\n",
    "medicine.to_csv('/dbfs/mnt/lsde/group14/topic/humanites_topic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e560b6dd-65df-4056-a209-d40c876d67b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)\n",
    "#save html generated by LDA model\n",
    "pyLDAvis.save_html(p, '/dbfs/Filestore/humanites.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1cd2ce9f-0a32-4aed-b7c8-7e99c0a8c61c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Index([&#39;Unnamed: 0&#39;, &#39;year&#39;, &#39;title&#39;, &#39;subject&#39;, &#39;content&#39;, &#39;area&#39;,\n",
       "       &#39;subdiscipline&#39;, &#39;content_processed&#39;, &#39;topic&#39;],\n",
       "      dtype=&#39;object&#39;)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Index([&#39;Unnamed: 0&#39;, &#39;year&#39;, &#39;title&#39;, &#39;subject&#39;, &#39;content&#39;, &#39;area&#39;,\n       &#39;subdiscipline&#39;, &#39;content_processed&#39;, &#39;topic&#39;],\n      dtype=&#39;object&#39;)\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-865234340589593&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> medicine <span class=\"ansi-blue-fg\">=</span> pd<span class=\"ansi-blue-fg\">.</span>read_csv<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/mnt/lsde/group14/topic/cs_topic.csv&#39;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> print<span class=\"ansi-blue-fg\">(</span>medicine<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>display<span class=\"ansi-blue-fg\">(</span>medicine<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/local_disk0/tmp/1666884335509-0/PythonShell.py</span> in <span class=\"ansi-cyan-fg\">display</span><span class=\"ansi-blue-fg\">(self, input, *args, **kwargs)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1421</span>             <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>sparkSession <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1422</span>                 <span class=\"ansi-green-fg\">raise</span> Exception<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;SparkSession is required for display(pandas.DataFrame).&#39;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1423</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>display<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>sparkSession<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>input<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1424</span>         elif type(input).__module__ == &#39;databricks.koalas.frame&#39; and type(\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1425</span>                 input).__name__ == &#39;DataFrame&#39;:\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/databricks/utils/instrumentation.py</span> in <span class=\"ansi-cyan-fg\">wrapper</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     40</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     41</span>             start_time <span class=\"ansi-blue-fg\">=</span> time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 42</span><span class=\"ansi-red-fg\">             </span>return_val <span class=\"ansi-blue-fg\">=</span> func<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     43</span>         <span class=\"ansi-green-fg\">except</span> Exception<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     44</span>             duration <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">(</span>time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-</span> start_time<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">*</span> <span class=\"ansi-cyan-fg\">1000</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    655</span>             <span class=\"ansi-red-fg\"># Create a DataFrame from pandas DataFrame.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    656</span>             return super(SparkSession, self).createDataFrame(\n",
       "<span class=\"ansi-green-fg\">--&gt; 657</span><span class=\"ansi-red-fg\">                 data, schema, samplingRatio, verifySchema)\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    658</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    659</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/pandas/conversion.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    309</span>                     <span class=\"ansi-green-fg\">raise</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    310</span>         data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_convert_from_pandas<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> timezone<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 311</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    312</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    313</span>     <span class=\"ansi-green-fg\">def</span> _convert_from_pandas<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> pdf<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> timezone<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_create_dataframe</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    688</span>                 rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromRDD<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    689</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 690</span><span class=\"ansi-red-fg\">                 </span>rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromLocal<span class=\"ansi-blue-fg\">(</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    691</span>             jrdd <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>SerDeUtil<span class=\"ansi-blue-fg\">.</span>toJavaArray<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">.</span>_to_java_object_rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    692</span>             jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>applySchemaToPythonRDD<span class=\"ansi-blue-fg\">(</span>jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">.</span>json<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_createFromLocal</span><span class=\"ansi-blue-fg\">(self, data, schema)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    472</span>         write temp files<span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    473</span>         &#34;&#34;&#34;\n",
       "<span class=\"ansi-green-fg\">--&gt; 474</span><span class=\"ansi-red-fg\">         </span>data<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_wrap_data_schema<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    475</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>parallelize<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    476</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_wrap_data_schema</span><span class=\"ansi-blue-fg\">(self, data, schema)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    451</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    452</span>         <span class=\"ansi-green-fg\">if</span> schema <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">or</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>list<span class=\"ansi-blue-fg\">,</span> tuple<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 453</span><span class=\"ansi-red-fg\">             </span>struct <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_inferSchemaFromList<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">=</span>schema<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    454</span>             converter <span class=\"ansi-blue-fg\">=</span> _create_converter<span class=\"ansi-blue-fg\">(</span>struct<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    455</span>             data <span class=\"ansi-blue-fg\">=</span> map<span class=\"ansi-blue-fg\">(</span>converter<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_inferSchemaFromList</span><span class=\"ansi-blue-fg\">(self, data, names)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    387</span>             warnings.warn(&#34;inferring schema from dict is deprecated,&#34;\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>                           &#34;please use pyspark.sql.Row instead&#34;)\n",
       "<span class=\"ansi-green-fg\">--&gt; 389</span><span class=\"ansi-red-fg\">         </span>schema <span class=\"ansi-blue-fg\">=</span> reduce<span class=\"ansi-blue-fg\">(</span>_merge_type<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>_infer_schema<span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">for</span> row <span class=\"ansi-green-fg\">in</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    390</span>         <span class=\"ansi-green-fg\">if</span> _has_nulltype<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    391</span>             <span class=\"ansi-green-fg\">raise</span> ValueError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Some of types cannot be determined after inferring&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">_merge_type</span><span class=\"ansi-blue-fg\">(a, b, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1104</span>         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1105</span>                                                   name=new_name(f.name)))\n",
       "<span class=\"ansi-green-fg\">-&gt; 1106</span><span class=\"ansi-red-fg\">                   for f in a.fields]\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1107</span>         names <span class=\"ansi-blue-fg\">=</span> set<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span>f<span class=\"ansi-blue-fg\">.</span>name <span class=\"ansi-green-fg\">for</span> f <span class=\"ansi-green-fg\">in</span> fields<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1108</span>         <span class=\"ansi-green-fg\">for</span> n <span class=\"ansi-green-fg\">in</span> nfs<span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">&lt;listcomp&gt;</span><span class=\"ansi-blue-fg\">(.0)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1104</span>         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1105</span>                                                   name=new_name(f.name)))\n",
       "<span class=\"ansi-green-fg\">-&gt; 1106</span><span class=\"ansi-red-fg\">                   for f in a.fields]\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1107</span>         names <span class=\"ansi-blue-fg\">=</span> set<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span>f<span class=\"ansi-blue-fg\">.</span>name <span class=\"ansi-green-fg\">for</span> f <span class=\"ansi-green-fg\">in</span> fields<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1108</span>         <span class=\"ansi-green-fg\">for</span> n <span class=\"ansi-green-fg\">in</span> nfs<span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">_merge_type</span><span class=\"ansi-blue-fg\">(a, b, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1097</span>     <span class=\"ansi-green-fg\">elif</span> type<span class=\"ansi-blue-fg\">(</span>a<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> type<span class=\"ansi-blue-fg\">(</span>b<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1098</span>         <span class=\"ansi-red-fg\"># TODO: type cast (such as int -&gt; long)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1099</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> TypeError<span class=\"ansi-blue-fg\">(</span>new_msg<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Can not merge type %s and %s&#34;</span> <span class=\"ansi-blue-fg\">%</span> <span class=\"ansi-blue-fg\">(</span>type<span class=\"ansi-blue-fg\">(</span>a<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> type<span class=\"ansi-blue-fg\">(</span>b<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1100</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1101</span>     <span class=\"ansi-red-fg\"># same type</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">TypeError</span>: field content_processed: Can not merge type &lt;class &#39;pyspark.sql.types.StringType&#39;&gt; and &lt;class &#39;pyspark.sql.types.DoubleType&#39;&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-865234340589593&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> medicine <span class=\"ansi-blue-fg\">=</span> pd<span class=\"ansi-blue-fg\">.</span>read_csv<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/mnt/lsde/group14/topic/cs_topic.csv&#39;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> print<span class=\"ansi-blue-fg\">(</span>medicine<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>display<span class=\"ansi-blue-fg\">(</span>medicine<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/local_disk0/tmp/1666884335509-0/PythonShell.py</span> in <span class=\"ansi-cyan-fg\">display</span><span class=\"ansi-blue-fg\">(self, input, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1421</span>             <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>sparkSession <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1422</span>                 <span class=\"ansi-green-fg\">raise</span> Exception<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;SparkSession is required for display(pandas.DataFrame).&#39;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1423</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>display<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>sparkSession<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>input<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1424</span>         elif type(input).__module__ == &#39;databricks.koalas.frame&#39; and type(\n<span class=\"ansi-green-intense-fg ansi-bold\">   1425</span>                 input).__name__ == &#39;DataFrame&#39;:\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/databricks/utils/instrumentation.py</span> in <span class=\"ansi-cyan-fg\">wrapper</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     40</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     41</span>             start_time <span class=\"ansi-blue-fg\">=</span> time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 42</span><span class=\"ansi-red-fg\">             </span>return_val <span class=\"ansi-blue-fg\">=</span> func<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     43</span>         <span class=\"ansi-green-fg\">except</span> Exception<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     44</span>             duration <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">(</span>time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-</span> start_time<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">*</span> <span class=\"ansi-cyan-fg\">1000</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    655</span>             <span class=\"ansi-red-fg\"># Create a DataFrame from pandas DataFrame.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    656</span>             return super(SparkSession, self).createDataFrame(\n<span class=\"ansi-green-fg\">--&gt; 657</span><span class=\"ansi-red-fg\">                 data, schema, samplingRatio, verifySchema)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    658</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    659</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/pandas/conversion.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    309</span>                     <span class=\"ansi-green-fg\">raise</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    310</span>         data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_convert_from_pandas<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> timezone<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 311</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    312</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    313</span>     <span class=\"ansi-green-fg\">def</span> _convert_from_pandas<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> pdf<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> timezone<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_create_dataframe</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    688</span>                 rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromRDD<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    689</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 690</span><span class=\"ansi-red-fg\">                 </span>rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromLocal<span class=\"ansi-blue-fg\">(</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    691</span>             jrdd <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>SerDeUtil<span class=\"ansi-blue-fg\">.</span>toJavaArray<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">.</span>_to_java_object_rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    692</span>             jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>applySchemaToPythonRDD<span class=\"ansi-blue-fg\">(</span>jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">.</span>json<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_createFromLocal</span><span class=\"ansi-blue-fg\">(self, data, schema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    472</span>         write temp files<span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    473</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 474</span><span class=\"ansi-red-fg\">         </span>data<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_wrap_data_schema<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    475</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>parallelize<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema\n<span class=\"ansi-green-intense-fg ansi-bold\">    476</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_wrap_data_schema</span><span class=\"ansi-blue-fg\">(self, data, schema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    451</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    452</span>         <span class=\"ansi-green-fg\">if</span> schema <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">or</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>list<span class=\"ansi-blue-fg\">,</span> tuple<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 453</span><span class=\"ansi-red-fg\">             </span>struct <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_inferSchemaFromList<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">=</span>schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    454</span>             converter <span class=\"ansi-blue-fg\">=</span> _create_converter<span class=\"ansi-blue-fg\">(</span>struct<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    455</span>             data <span class=\"ansi-blue-fg\">=</span> map<span class=\"ansi-blue-fg\">(</span>converter<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_inferSchemaFromList</span><span class=\"ansi-blue-fg\">(self, data, names)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span>             warnings.warn(&#34;inferring schema from dict is deprecated,&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>                           &#34;please use pyspark.sql.Row instead&#34;)\n<span class=\"ansi-green-fg\">--&gt; 389</span><span class=\"ansi-red-fg\">         </span>schema <span class=\"ansi-blue-fg\">=</span> reduce<span class=\"ansi-blue-fg\">(</span>_merge_type<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>_infer_schema<span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">for</span> row <span class=\"ansi-green-fg\">in</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    390</span>         <span class=\"ansi-green-fg\">if</span> _has_nulltype<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    391</span>             <span class=\"ansi-green-fg\">raise</span> ValueError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Some of types cannot be determined after inferring&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">_merge_type</span><span class=\"ansi-blue-fg\">(a, b, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1104</span>         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n<span class=\"ansi-green-intense-fg ansi-bold\">   1105</span>                                                   name=new_name(f.name)))\n<span class=\"ansi-green-fg\">-&gt; 1106</span><span class=\"ansi-red-fg\">                   for f in a.fields]\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1107</span>         names <span class=\"ansi-blue-fg\">=</span> set<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span>f<span class=\"ansi-blue-fg\">.</span>name <span class=\"ansi-green-fg\">for</span> f <span class=\"ansi-green-fg\">in</span> fields<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1108</span>         <span class=\"ansi-green-fg\">for</span> n <span class=\"ansi-green-fg\">in</span> nfs<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">&lt;listcomp&gt;</span><span class=\"ansi-blue-fg\">(.0)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1104</span>         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n<span class=\"ansi-green-intense-fg ansi-bold\">   1105</span>                                                   name=new_name(f.name)))\n<span class=\"ansi-green-fg\">-&gt; 1106</span><span class=\"ansi-red-fg\">                   for f in a.fields]\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1107</span>         names <span class=\"ansi-blue-fg\">=</span> set<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span>f<span class=\"ansi-blue-fg\">.</span>name <span class=\"ansi-green-fg\">for</span> f <span class=\"ansi-green-fg\">in</span> fields<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1108</span>         <span class=\"ansi-green-fg\">for</span> n <span class=\"ansi-green-fg\">in</span> nfs<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">_merge_type</span><span class=\"ansi-blue-fg\">(a, b, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1097</span>     <span class=\"ansi-green-fg\">elif</span> type<span class=\"ansi-blue-fg\">(</span>a<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> type<span class=\"ansi-blue-fg\">(</span>b<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1098</span>         <span class=\"ansi-red-fg\"># TODO: type cast (such as int -&gt; long)</span>\n<span class=\"ansi-green-fg\">-&gt; 1099</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> TypeError<span class=\"ansi-blue-fg\">(</span>new_msg<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Can not merge type %s and %s&#34;</span> <span class=\"ansi-blue-fg\">%</span> <span class=\"ansi-blue-fg\">(</span>type<span class=\"ansi-blue-fg\">(</span>a<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> type<span class=\"ansi-blue-fg\">(</span>b<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1100</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1101</span>     <span class=\"ansi-red-fg\"># same type</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: field content_processed: Can not merge type &lt;class &#39;pyspark.sql.types.StringType&#39;&gt; and &lt;class &#39;pyspark.sql.types.DoubleType&#39;&gt;</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">TypeError</span>: field content_processed: Can not merge type &lt;class &#39;pyspark.sql.types.StringType&#39;&gt; and &lt;class &#39;pyspark.sql.types.DoubleType&#39;&gt;",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "medicine = pd.read_csv('/dbfs/mnt/lsde/group14/topic/cs_topic.csv')\n",
    "print(medicine.columns)\n",
    "#visiualize topic trends of each area\n",
    "display(medicine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e3f6130d-7615-4464-8e46-a32860be8275",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[1]: True</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[1]: True</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.mv('/mnt/lsde/group14/topic/topic.tar.gz', '/FileStore/Group14/')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ldavisual",
   "notebookOrigID": 150296596912311,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
